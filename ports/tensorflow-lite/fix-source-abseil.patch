diff --git a/tensorflow/compiler/xla/runtime/custom_call.h b/tensorflow/compiler/xla/runtime/custom_call.h
index 400edef0..f01508ff 100644
--- a/tensorflow/compiler/xla/runtime/custom_call.h
+++ b/tensorflow/compiler/xla/runtime/custom_call.h
@@ -834,7 +834,7 @@ template <typename T, CustomCall::RuntimeChecks checks>
 struct Decode<internal::Value<T>, checks> {
   ABSL_ATTRIBUTE_ALWAYS_INLINE static FailureOr<T> call(
       DecodingOffsets& offsets, DecodingContext& ctx) {
-    return std::any_cast<T>(ctx.values[offsets.values++]);
+    return absl::any_cast<T>(ctx.values[offsets.values++]);
   }
 };
 
diff --git a/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc b/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc
index 7681cba5..5bd5ff4d 100644
--- a/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc
+++ b/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc
@@ -1526,7 +1526,7 @@ GpuCompiler::CompileAheadOfTime(std::unique_ptr<HloModuleGroup> module_group,
   std::vector<std::unique_ptr<AotCompilationResult>> results;
 
   std::any target_config = options.target_config();
-  auto* gpu_target_config = std::any_cast<GpuTargetConfig>(&target_config);
+  auto* gpu_target_config = absl::any_cast<GpuTargetConfig>(&target_config);
   CHECK(gpu_target_config != nullptr || options.executor() != nullptr);
 
   for (const auto& module : modules) {
@@ -1536,7 +1536,7 @@ GpuCompiler::CompileAheadOfTime(std::unique_ptr<HloModuleGroup> module_group,
     CompileModuleResults compile_module_results;
 
     const std::any& target_config = options.target_config();
-    auto* gpu_target_config = std::any_cast<GpuTargetConfig>(&target_config);
+    auto* gpu_target_config = absl::any_cast<GpuTargetConfig>(&target_config);
 
     if (gpu_target_config) {
       // CUDA "CC" major value, -1 if not available.
diff --git a/tensorflow/lite/delegates/gpu/common/operations.h b/tensorflow/lite/delegates/gpu/common/operations.h
index 37d8dfa0..588d2fc2 100644
--- a/tensorflow/lite/delegates/gpu/common/operations.h
+++ b/tensorflow/lite/delegates/gpu/common/operations.h
@@ -119,8 +119,8 @@ std::string ToString(enum OperationType op);
 OperationType OperationTypeFromString(const std::string& name);
 
 template <DataType DataTypeT, typename t>
-using TensorOrScalarBase = std::variant<std::monostate, Tensor<HWC, DataTypeT>,
-                                        Tensor<Linear, DataTypeT>, t>;
+using TensorOrScalarBase = absl::variant<std::monostate, Tensor<HWC, DataTypeT>,
+                                         Tensor<Linear, DataTypeT>, t>;
 
 using TensorOrScalar = TensorOrScalarBase<DataType::FLOAT32, float>;
 
diff --git a/tensorflow/lite/delegates/gpu/common/selectors/operation_selector.cc b/tensorflow/lite/delegates/gpu/common/selectors/operation_selector.cc
index 655c7bba..88027ae6 100644
--- a/tensorflow/lite/delegates/gpu/common/selectors/operation_selector.cc
+++ b/tensorflow/lite/delegates/gpu/common/selectors/operation_selector.cc
@@ -310,7 +310,7 @@ absl::Status CreateElementwiseTwoInputWithOneConstant(
     const GpuInfo& gpu_info, const OperationDef& op_def, OperationType op_type,
     const Node& node, const Value* input, const Value* output,
     std::unique_ptr<GPUOperation>* gpu_op) {
-  auto attr = std::any_cast<ElementwiseAttributesBase<DataTypeT, T>>(
+  auto attr = absl::any_cast<ElementwiseAttributesBase<DataTypeT, T>>(
       node.operation.attributes);
   GPUOperation operation;
   if (input->tensor.shape != output->tensor.shape) {
diff --git a/tensorflow/lite/delegates/gpu/common/tasks/elementwise.cc b/tensorflow/lite/delegates/gpu/common/tasks/elementwise.cc
index ff8cf35f..615827e5 100644
--- a/tensorflow/lite/delegates/gpu/common/tasks/elementwise.cc
+++ b/tensorflow/lite/delegates/gpu/common/tasks/elementwise.cc
@@ -340,11 +340,11 @@ ElementwiseDescriptor CreateElementwiseDesc(
     const GpuInfo& gpu_info, const OperationDef& definition,
     const OperationType& op_type,
     const ElementwiseAttributesBase<DataTypeT, T>& attr) {
-  const T* scalar = std::get_if<T>(&attr.param);
+  const T* scalar = absl::get_if<T>(&attr.param);
   const auto* linear_tensor =
-      std::get_if<tflite::gpu::Tensor<Linear, DataTypeT>>(&attr.param);
+      absl::get_if<tflite::gpu::Tensor<Linear, DataTypeT>>(&attr.param);
   const auto* hwc_tensor =
-      std::get_if<tflite::gpu::Tensor<HWC, DataTypeT>>(&attr.param);
+      absl::get_if<tflite::gpu::Tensor<HWC, DataTypeT>>(&attr.param);
 
   if (scalar) {
     return CreateElementwiseOneRuntimeOneScalar(definition, op_type, *scalar,
diff --git a/tensorflow/lite/delegates/gpu/common/tasks/special/conv_pointwise.cc b/tensorflow/lite/delegates/gpu/common/tasks/special/conv_pointwise.cc
index b8c4d05c..bee97ac8 100644
--- a/tensorflow/lite/delegates/gpu/common/tasks/special/conv_pointwise.cc
+++ b/tensorflow/lite/delegates/gpu/common/tasks/special/conv_pointwise.cc
@@ -126,7 +126,7 @@ absl::Status IsReduceSumNode(const GraphFloat32& graph, Node* node,
   RETURN_IF_ERROR(
       IsNode(graph, OperationType::REDUCE_SUM, 1, 1, node, node_context));
   auto reduce_attr =
-      std::any_cast<ReduceAttributes>(node_context->node->operation.attributes);
+      absl::any_cast<ReduceAttributes>(node_context->node->operation.attributes);
   if (reduce_attr.dims != std::set<Axis>{Axis::CHANNELS}) {
     return absl::InternalError(
         "Expected reduce_sum node with channels reduction.");
diff --git a/tensorflow/lite/delegates/gpu/gl/compiler.cc b/tensorflow/lite/delegates/gpu/gl/compiler.cc
index d6e670e0..bafa99fb 100644
--- a/tensorflow/lite/delegates/gpu/gl/compiler.cc
+++ b/tensorflow/lite/delegates/gpu/gl/compiler.cc
@@ -196,7 +196,7 @@ class CompilerImpl : public Compiler {
     // Prepare readonly objects and check whether object types are supported.
     for (auto node : compiled_graph_.nodes()) {
       auto& attr =
-          std::any_cast<CompiledNodeAttributes&>(node->operation.attributes);
+          absl::any_cast<CompiledNodeAttributes&>(node->operation.attributes);
 
       // Set workload explicitly.
       if (attr.code.workload == uint3()) {
@@ -251,7 +251,7 @@ class CompilerImpl : public Compiler {
     ShaderCodegen codegen(options_, gpu_info_);
     for (auto node : compiled_graph_.nodes()) {
       auto& attr =
-          std::any_cast<CompiledNodeAttributes&>(node->operation.attributes);
+          absl::any_cast<CompiledNodeAttributes&>(node->operation.attributes);
       if (attr.code.source_code.empty()) {
         // noop. Skip this node.
         continue;
diff --git a/tensorflow/lite/delegates/gpu/gl/compiler/fuse_auto_input.cc b/tensorflow/lite/delegates/gpu/gl/compiler/fuse_auto_input.cc
index 761fb8b4..c7a7de9a 100644
--- a/tensorflow/lite/delegates/gpu/gl/compiler/fuse_auto_input.cc
+++ b/tensorflow/lite/delegates/gpu/gl/compiler/fuse_auto_input.cc
@@ -46,7 +46,7 @@ std::pair<std::string, std::string> MakeDataReplacement(int n, int k) {
 
 TransformResult FuseAutoInput::ApplyToNode(Node* node, GraphFloat32* graph) {
   auto& node_attr =
-      std::any_cast<CompiledNodeAttributes&>(node->operation.attributes);
+      absl::any_cast<CompiledNodeAttributes&>(node->operation.attributes);
   auto& node_code = node_attr.code;
 
   if (node_code.input != IOStructure::AUTO) {
@@ -75,7 +75,7 @@ TransformResult FuseAutoInput::ApplyToNode(Node* node, GraphFloat32* graph) {
     if (graph->FindOutputs(input_producer->id).size() != 1) {
       continue;  // input node has more than one output
     }
-    auto& input_producer_attr = std::any_cast<const CompiledNodeAttributes&>(
+    auto& input_producer_attr = absl::any_cast<const CompiledNodeAttributes&>(
         input_producer->operation.attributes);
     if (input_producer_attr.code.output != IOStructure::AUTO) {
       continue;
@@ -143,7 +143,7 @@ TransformResult FuseAutoInput::ApplyToNode(Node* node, GraphFloat32* graph) {
   for (auto input_and_num : nodes_to_fuse) {
     auto& input = input_and_num.first;
     auto& attr =
-        std::any_cast<CompiledNodeAttributes&>(input->operation.attributes);
+        absl::any_cast<CompiledNodeAttributes&>(input->operation.attributes);
     auto super_inputs = graph->FindInputs(input->id);
 
     // Replace all internal references in the input source code. For example:
diff --git a/tensorflow/lite/delegates/gpu/gl/compiler/fuse_inline.cc b/tensorflow/lite/delegates/gpu/gl/compiler/fuse_inline.cc
index f227ab21..486d4544 100644
--- a/tensorflow/lite/delegates/gpu/gl/compiler/fuse_inline.cc
+++ b/tensorflow/lite/delegates/gpu/gl/compiler/fuse_inline.cc
@@ -40,9 +40,9 @@ TransformResult FuseAutoOutputWithInline::ApplyToNodesSequence(
   Node* node1 = sequence.front();
   Node* node2 = sequence.back();
   auto& attr1 =
-      std::any_cast<CompiledNodeAttributes&>(node1->operation.attributes);
+      absl::any_cast<CompiledNodeAttributes&>(node1->operation.attributes);
   auto& attr2 =
-      std::any_cast<CompiledNodeAttributes&>(node2->operation.attributes);
+      absl::any_cast<CompiledNodeAttributes&>(node2->operation.attributes);
 
   if (attr1.code.output != IOStructure::AUTO ||
       graph->FindInputs(node2->id).size() != 1 ||
diff --git a/tensorflow/lite/delegates/gpu/gl/compiler/fuse_inplace.cc b/tensorflow/lite/delegates/gpu/gl/compiler/fuse_inplace.cc
index 1e27404b..b7719c49 100644
--- a/tensorflow/lite/delegates/gpu/gl/compiler/fuse_inplace.cc
+++ b/tensorflow/lite/delegates/gpu/gl/compiler/fuse_inplace.cc
@@ -81,7 +81,7 @@ class InplaceCodeRewrite : public InlineRewrite {
 TransformResult RemoveUnusedInplaceUpdates::ApplyToNode(Node* node,
                                                         GraphFloat32* graph) {
   auto& attr =
-      std::any_cast<CompiledNodeAttributes&>(node->operation.attributes);
+      absl::any_cast<CompiledNodeAttributes&>(node->operation.attributes);
   // Remove inplace block by rewriting to empty string.
   EmptyInplaceRewrite rewrite;
   TextPreprocessor preprocessor('$', true);
@@ -100,9 +100,9 @@ TransformResult FuseInplaceUpdate::ApplyToNodesSequence(
   Node* node1 = sequence.front();
   Node* node2 = sequence.back();
   auto& attr1 =
-      std::any_cast<CompiledNodeAttributes&>(node1->operation.attributes);
+      absl::any_cast<CompiledNodeAttributes&>(node1->operation.attributes);
   auto& attr2 =
-      std::any_cast<CompiledNodeAttributes&>(node2->operation.attributes);
+      absl::any_cast<CompiledNodeAttributes&>(node2->operation.attributes);
 
   if (graph->FindInputs(node2->id).size() != 1 ||
       graph->FindOutputs(node2->id).size() != 1 ||
diff --git a/tensorflow/lite/delegates/gpu/gl/kernels/add.cc b/tensorflow/lite/delegates/gpu/gl/kernels/add.cc
index a14d7f24..0a01997c 100644
--- a/tensorflow/lite/delegates/gpu/gl/kernels/add.cc
+++ b/tensorflow/lite/delegates/gpu/gl/kernels/add.cc
@@ -41,12 +41,12 @@ class Add : public NodeShader {
  public:
   absl::Status GenerateCode(const GenerationContext& ctx,
                             GeneratedCode* generated_code) const final {
-    const auto& attr = std::any_cast<const ElementwiseAttributes&>(ctx.op_attr);
-    auto adds = std::get_if<Tensor<Linear, DataType::FLOAT32>>(&attr.param);
-    auto scalar = std::get_if<float>(&attr.param);
+    const auto& attr = absl::any_cast<const ElementwiseAttributes&>(ctx.op_attr);
+    auto adds = absl::get_if<Tensor<Linear, DataType::FLOAT32>>(&attr.param);
+    auto scalar = absl::get_if<float>(&attr.param);
 
     const auto* hwc_tensor =
-        std::get_if<Tensor<HWC, DataType::FLOAT32>>(&attr.param);
+        absl::get_if<Tensor<HWC, DataType::FLOAT32>>(&attr.param);
 
     if (hwc_tensor) {
       std::string code;
@@ -69,7 +69,7 @@ class Add : public NodeShader {
                 uint3(hwc_tensor->shape.w, hwc_tensor->shape.h,
                       DivideRoundUp(hwc_tensor->shape.c, 4)),
                 ConvertToPHWC4(
-                    std::get<Tensor<HWC, DataType::FLOAT32>>(attr.param)))}},
+                    absl::get<Tensor<HWC, DataType::FLOAT32>>(attr.param)))}},
           /*shared_variables=*/{},
           // Declare workload explicitly because shader depends on gid.z.
           /*workload=*/
diff --git a/tensorflow/lite/delegates/gpu/gl/kernels/concat.cc b/tensorflow/lite/delegates/gpu/gl/kernels/concat.cc
index 0513c8ec..c737b490 100644
--- a/tensorflow/lite/delegates/gpu/gl/kernels/concat.cc
+++ b/tensorflow/lite/delegates/gpu/gl/kernels/concat.cc
@@ -37,7 +37,7 @@ namespace {
 class AlignedConcatByChannels : public NodeShader {
  public:
   static bool IsSupported(const GenerationContext& ctx) {
-    const auto& attr = std::any_cast<const ConcatAttributes&>(ctx.op_attr);
+    const auto& attr = absl::any_cast<const ConcatAttributes&>(ctx.op_attr);
 
     // Implementation supports concatenation by channels only.
     if (attr.axis != Axis::CHANNELS) return false;
@@ -95,7 +95,7 @@ class AlignedConcatByChannels : public NodeShader {
 class ConcatByAnyChannel : public NodeShader {
  public:
   static bool IsSupported(const GenerationContext& ctx) {
-    const auto& attr = std::any_cast<const ConcatAttributes&>(ctx.op_attr);
+    const auto& attr = absl::any_cast<const ConcatAttributes&>(ctx.op_attr);
 
     // Implementation supports concatenation by channels only.
     if (attr.axis != Axis::CHANNELS) return false;
@@ -308,7 +308,7 @@ vec4 val = vec4(0.0f);
 class FlatConcatByHeight : public NodeShader {
  public:
   static bool IsSupported(const GenerationContext& ctx) {
-    const auto& attr = std::any_cast<const ConcatAttributes&>(ctx.op_attr);
+    const auto& attr = absl::any_cast<const ConcatAttributes&>(ctx.op_attr);
 
     // Implementation supports concatenation by height only.
     if (attr.axis != Axis::HEIGHT) return false;
@@ -367,7 +367,7 @@ class FlatConcatByHeight : public NodeShader {
 class FlatConcatByWidth : public NodeShader {
  public:
   static bool IsSupported(const GenerationContext& ctx) {
-    const auto& attr = std::any_cast<const ConcatAttributes&>(ctx.op_attr);
+    const auto& attr = absl::any_cast<const ConcatAttributes&>(ctx.op_attr);
 
     // Implementation supports concatenation by width only.
     if (attr.axis != Axis::WIDTH) return false;
diff --git a/tensorflow/lite/delegates/gpu/gl/kernels/conv.cc b/tensorflow/lite/delegates/gpu/gl/kernels/conv.cc
index 8522ea25..0e67fe10 100644
--- a/tensorflow/lite/delegates/gpu/gl/kernels/conv.cc
+++ b/tensorflow/lite/delegates/gpu/gl/kernels/conv.cc
@@ -47,7 +47,7 @@ class Convolution : public NodeShader {
           "Convolution does not support more than 1 runtime tensor");
     }
     const auto& attr =
-        std::any_cast<const Convolution2DAttributes&>(ctx.op_attr);
+        absl::any_cast<const Convolution2DAttributes&>(ctx.op_attr);
     if (attr.groups != 1) {
       return absl::UnimplementedError(
           "Convolution does not support more than 1 group");
@@ -179,7 +179,7 @@ class Convolution1x1 : public NodeShader {
           "Convolution does not support more than 1 runtime tensor");
     }
     const auto& attr =
-        std::any_cast<const Convolution2DAttributes&>(ctx.op_attr);
+        absl::any_cast<const Convolution2DAttributes&>(ctx.op_attr);
     if (attr.weights.shape.h != 1 || attr.weights.shape.w != 1) {
       return absl::UnimplementedError("Height and width should be 1.");
     }
diff --git a/tensorflow/lite/delegates/gpu/gl/kernels/depthwise_conv.cc b/tensorflow/lite/delegates/gpu/gl/kernels/depthwise_conv.cc
index 627aeeec..b86eefff 100644
--- a/tensorflow/lite/delegates/gpu/gl/kernels/depthwise_conv.cc
+++ b/tensorflow/lite/delegates/gpu/gl/kernels/depthwise_conv.cc
@@ -46,7 +46,7 @@ class DepthwiseConvolution : public NodeShader {
           "DepthWise Convolution does not support more than 1 runtime tensor");
     }
     const auto& attr =
-        std::any_cast<const DepthwiseConvolution2DAttributes&>(ctx.op_attr);
+        absl::any_cast<const DepthwiseConvolution2DAttributes&>(ctx.op_attr);
     auto weights = attr.weights.shape;
     const int offsets_count = weights.h * weights.w;
     const bool offsets_count_too_large = offsets_count > kMaxConstArraySize;
diff --git a/tensorflow/lite/delegates/gpu/gl/kernels/elementwise.cc b/tensorflow/lite/delegates/gpu/gl/kernels/elementwise.cc
index db6714b0..e65ea47e 100644
--- a/tensorflow/lite/delegates/gpu/gl/kernels/elementwise.cc
+++ b/tensorflow/lite/delegates/gpu/gl/kernels/elementwise.cc
@@ -168,10 +168,10 @@ class ElementwiseTwoArguments : public NodeShader {
       argument1 = "$input_data_1[0, 0, gid.z]$";
     } else {  // Scalar of const vector case
       const auto& attr =
-          std::any_cast<const ElementwiseAttributes&>(ctx.op_attr);
+          absl::any_cast<const ElementwiseAttributes&>(ctx.op_attr);
       const auto* tensor =
-          std::get_if<Tensor<Linear, DataType::FLOAT32>>(&attr.param);
-      const auto* scalar = std::get_if<float>(&attr.param);
+          absl::get_if<Tensor<Linear, DataType::FLOAT32>>(&attr.param);
+      const auto* scalar = absl::get_if<float>(&attr.param);
       if (!tensor && !scalar) {
         return absl::InvalidArgumentError(
             "Couldn't read scalar of const vector data from the attributes.");
diff --git a/tensorflow/lite/delegates/gpu/gl/kernels/fully_connected.cc b/tensorflow/lite/delegates/gpu/gl/kernels/fully_connected.cc
index 7dacd3e6..256a16b6 100644
--- a/tensorflow/lite/delegates/gpu/gl/kernels/fully_connected.cc
+++ b/tensorflow/lite/delegates/gpu/gl/kernels/fully_connected.cc
@@ -40,7 +40,7 @@ class FullyConnectedBuffers : public NodeShader {
   absl::Status GenerateCode(const GenerationContext& ctx,
                             GeneratedCode* generated_code) const final {
     const auto& attr =
-        std::any_cast<const FullyConnectedAttributes&>(ctx.op_attr);
+        absl::any_cast<const FullyConnectedAttributes&>(ctx.op_attr);
 
     const int src_depth = DivideRoundUp(attr.weights.shape.i, 4);
     const int dst_depth = DivideRoundUp(attr.weights.shape.o, 4);
diff --git a/tensorflow/lite/delegates/gpu/gl/kernels/max_unpooling.cc b/tensorflow/lite/delegates/gpu/gl/kernels/max_unpooling.cc
index d59a3e7f..d993a8fc 100644
--- a/tensorflow/lite/delegates/gpu/gl/kernels/max_unpooling.cc
+++ b/tensorflow/lite/delegates/gpu/gl/kernels/max_unpooling.cc
@@ -39,7 +39,7 @@ class MaxUnpooling : public NodeShader {
   absl::Status GenerateCode(const GenerationContext& ctx,
                             GeneratedCode* generated_code) const final {
     const auto& attr =
-        std::any_cast<const MaxUnpooling2DAttributes&>(ctx.op_attr);
+        absl::any_cast<const MaxUnpooling2DAttributes&>(ctx.op_attr);
     std::vector<Variable> parameters = {
         {"stride", int2(attr.strides.w, attr.strides.h)},
         {"offset", int2(attr.padding.prepended.w, attr.padding.prepended.h)},
diff --git a/tensorflow/lite/delegates/gpu/gl/kernels/mean.cc b/tensorflow/lite/delegates/gpu/gl/kernels/mean.cc
index fbb37e0e..ea3edd01 100644
--- a/tensorflow/lite/delegates/gpu/gl/kernels/mean.cc
+++ b/tensorflow/lite/delegates/gpu/gl/kernels/mean.cc
@@ -242,7 +242,7 @@ class Mean : public NodeShader {
  public:
   absl::Status GenerateCode(const GenerationContext& ctx,
                             GeneratedCode* generated_code) const final {
-    const auto& attr = std::any_cast<const MeanAttributes&>(ctx.op_attr);
+    const auto& attr = absl::any_cast<const MeanAttributes&>(ctx.op_attr);
     if (attr.dims != std::set<Axis>({Axis::HEIGHT, Axis::WIDTH})) {
       return absl::InvalidArgumentError(
           "Mean calculation is supported only for height and width.");
diff --git a/tensorflow/lite/delegates/gpu/gl/kernels/mul.cc b/tensorflow/lite/delegates/gpu/gl/kernels/mul.cc
index fb309862..d6a3fe7e 100644
--- a/tensorflow/lite/delegates/gpu/gl/kernels/mul.cc
+++ b/tensorflow/lite/delegates/gpu/gl/kernels/mul.cc
@@ -90,11 +90,11 @@ absl::Status GenerateMultiplyRuntimeTensorCode(
 
 absl::Status GenerateMultiplyConstantTensorCode(
     const NodeShader::GenerationContext& ctx, GeneratedCode* generated_code) {
-  const auto& attr = std::any_cast<const ElementwiseAttributes&>(ctx.op_attr);
+  const auto& attr = absl::any_cast<const ElementwiseAttributes&>(ctx.op_attr);
 
-  if (std::holds_alternative<float>(attr.param)) {
+  if (absl::holds_alternative<float>(attr.param)) {
     *generated_code = {
-        /*parameters=*/{{"scalar", std::get<float>(attr.param)}},
+        /*parameters=*/{{"scalar", absl::get<float>(attr.param)}},
         /*objects=*/{},
         /*shared_variables=*/{},
         /*workload=*/uint3(),
@@ -106,13 +106,13 @@ absl::Status GenerateMultiplyConstantTensorCode(
     return absl::OkStatus();
   }
 
-  if (std::holds_alternative<Tensor<Linear, DataType::FLOAT32>>(attr.param)) {
+  if (absl::holds_alternative<Tensor<Linear, DataType::FLOAT32>>(attr.param)) {
     *generated_code = {
         /*parameters=*/{},
         /*objects=*/
         {{"mul_buffer",
           MakeReadonlyObject(
-              std::get<Tensor<Linear, DataType::FLOAT32>>(attr.param).data)}},
+              absl::get<Tensor<Linear, DataType::FLOAT32>>(attr.param).data)}},
         /*shared_variables=*/{},
         // Declare workload explicitly because shader depends on gid.z.
         /*workload=*/
@@ -127,9 +127,9 @@ absl::Status GenerateMultiplyConstantTensorCode(
     return absl::OkStatus();
   }
 
-  if (std::holds_alternative<Tensor<HWC, DataType::FLOAT32>>(attr.param)) {
+  if (absl::holds_alternative<Tensor<HWC, DataType::FLOAT32>>(attr.param)) {
     bool single_channel_mask =
-        std::get<Tensor<HWC, DataType::FLOAT32>>(attr.param).shape.c == 1;
+        absl::get<Tensor<HWC, DataType::FLOAT32>>(attr.param).shape.c == 1;
     std::string source;
     if (single_channel_mask) {
       source = "vec4 const_val = $hwc_buffer[gid.x, gid.y, 0]$;";
@@ -157,7 +157,7 @@ absl::Status GenerateMultiplyConstantTensorCode(
                     static_cast<int>(ctx.input_shapes[0][1]),
                     DivideRoundUp(static_cast<int>(ctx.input_shapes[0][3]), 4)),
               ConvertToPHWC4(
-                  std::get<Tensor<HWC, DataType::FLOAT32>>(attr.param)))}},
+                  absl::get<Tensor<HWC, DataType::FLOAT32>>(attr.param)))}},
         /*shared_variables=*/{},
         // Declare workload explicitly because shader depends on gid.z.
         /*workload=*/
diff --git a/tensorflow/lite/delegates/gpu/gl/kernels/pad.cc b/tensorflow/lite/delegates/gpu/gl/kernels/pad.cc
index 537da8c9..66eeb2ae 100644
--- a/tensorflow/lite/delegates/gpu/gl/kernels/pad.cc
+++ b/tensorflow/lite/delegates/gpu/gl/kernels/pad.cc
@@ -39,7 +39,7 @@ class Pad : public NodeShader {
  public:
   absl::Status GenerateCode(const GenerationContext& ctx,
                             GeneratedCode* generated_code) const final {
-    const auto& attr = std::any_cast<const PadAttributes&>(ctx.op_attr);
+    const auto& attr = absl::any_cast<const PadAttributes&>(ctx.op_attr);
 
     if (attr.type != PaddingContentType::ZEROS &&
         attr.type != PaddingContentType::REFLECT) {
diff --git a/tensorflow/lite/delegates/gpu/gl/kernels/pooling.cc b/tensorflow/lite/delegates/gpu/gl/kernels/pooling.cc
index ba746f6b..bc662957 100644
--- a/tensorflow/lite/delegates/gpu/gl/kernels/pooling.cc
+++ b/tensorflow/lite/delegates/gpu/gl/kernels/pooling.cc
@@ -178,7 +178,7 @@ class Pooling : public NodeShader {
  public:
   absl::Status GenerateCode(const GenerationContext& ctx,
                             GeneratedCode* generated_code) const final {
-    const auto& attr = std::any_cast<const Pooling2DAttributes&>(ctx.op_attr);
+    const auto& attr = absl::any_cast<const Pooling2DAttributes&>(ctx.op_attr);
     switch (attr.type) {
       case PoolingType::AVERAGE:
         return GenerateAveragePoolingCode(attr, ctx, generated_code);
diff --git a/tensorflow/lite/delegates/gpu/gl/kernels/prelu.cc b/tensorflow/lite/delegates/gpu/gl/kernels/prelu.cc
index 58882ba1..c71579ea 100644
--- a/tensorflow/lite/delegates/gpu/gl/kernels/prelu.cc
+++ b/tensorflow/lite/delegates/gpu/gl/kernels/prelu.cc
@@ -40,8 +40,8 @@ class PReLULinearAlpha : public NodeShader {
  public:
   absl::Status GenerateCode(const GenerationContext& ctx,
                             GeneratedCode* generated_code) const final {
-    const auto& attr = std::any_cast<const PReLUAttributes&>(ctx.op_attr);
-    auto alpha = std::get_if<Tensor<Linear, DataType::FLOAT32>>(&attr.alpha);
+    const auto& attr = absl::any_cast<const PReLUAttributes&>(ctx.op_attr);
+    auto alpha = absl::get_if<Tensor<Linear, DataType::FLOAT32>>(&attr.alpha);
     if (!alpha) {
       return absl::InvalidArgumentError("Alpha is missing");
     }
@@ -75,8 +75,8 @@ class PReLUFull : public NodeShader {
  public:
   absl::Status GenerateCode(const GenerationContext& ctx,
                             GeneratedCode* generated_code) const final {
-    const auto& attr = std::any_cast<const PReLUAttributes&>(ctx.op_attr);
-    auto alpha = std::get_if<Tensor<HWC, DataType::FLOAT32>>(&attr.alpha);
+    const auto& attr = absl::any_cast<const PReLUAttributes&>(ctx.op_attr);
+    auto alpha = absl::get_if<Tensor<HWC, DataType::FLOAT32>>(&attr.alpha);
     if (!alpha) {
       return absl::InvalidArgumentError("Alpha is missing");
     }
@@ -118,8 +118,8 @@ class PReLU : public NodeShader {
  public:
   absl::Status GenerateCode(const GenerationContext& ctx,
                             GeneratedCode* generated_code) const final {
-    const auto& attr = std::any_cast<const PReLUAttributes&>(ctx.op_attr);
-    auto* alpha = std::get_if<Tensor<HWC, DataType::FLOAT32>>(&attr.alpha);
+    const auto& attr = absl::any_cast<const PReLUAttributes&>(ctx.op_attr);
+    auto* alpha = absl::get_if<Tensor<HWC, DataType::FLOAT32>>(&attr.alpha);
     return alpha ? full_.GenerateCode(ctx, generated_code)
                  : linear_.GenerateCode(ctx, generated_code);
   }
diff --git a/tensorflow/lite/delegates/gpu/gl/kernels/quantize_and_dequantize.cc b/tensorflow/lite/delegates/gpu/gl/kernels/quantize_and_dequantize.cc
index 80f03dde..3af047a0 100644
--- a/tensorflow/lite/delegates/gpu/gl/kernels/quantize_and_dequantize.cc
+++ b/tensorflow/lite/delegates/gpu/gl/kernels/quantize_and_dequantize.cc
@@ -42,7 +42,7 @@ value_0 = value_0 * vec4($quant_scale$) + vec4($quant_min$);
 )";
 
     const auto& attr =
-        std::any_cast<const QuantizeAndDequantizeAttributes&>(ctx.op_attr);
+        absl::any_cast<const QuantizeAndDequantizeAttributes&>(ctx.op_attr);
     *generated_code = {
         /*parameters=*/{{"quant_min", attr.min},
                         {"quant_max", attr.max},
diff --git a/tensorflow/lite/delegates/gpu/gl/kernels/relu.cc b/tensorflow/lite/delegates/gpu/gl/kernels/relu.cc
index 6d05ea89..2ba5f107 100644
--- a/tensorflow/lite/delegates/gpu/gl/kernels/relu.cc
+++ b/tensorflow/lite/delegates/gpu/gl/kernels/relu.cc
@@ -38,7 +38,7 @@ class ReLU : public NodeShader {
  public:
   absl::Status GenerateCode(const GenerationContext& ctx,
                             GeneratedCode* generated_code) const final {
-    const auto& attr = std::any_cast<const ReLUAttributes&>(ctx.op_attr);
+    const auto& attr = absl::any_cast<const ReLUAttributes&>(ctx.op_attr);
     // clamp(value, min(0, alpha * value), clip)
     std::vector<Variable> params;
     std::string min;
diff --git a/tensorflow/lite/delegates/gpu/gl/kernels/reshape.cc b/tensorflow/lite/delegates/gpu/gl/kernels/reshape.cc
index 899e7a1f..2bab3e5d 100644
--- a/tensorflow/lite/delegates/gpu/gl/kernels/reshape.cc
+++ b/tensorflow/lite/delegates/gpu/gl/kernels/reshape.cc
@@ -44,7 +44,7 @@ class Reshape : public NodeShader {
       return absl::InvalidArgumentError(
           "Number of elements in input & output tensors don't match.");
     }
-    const auto& attr = std::any_cast<const ReshapeAttributes&>(ctx.op_attr);
+    const auto& attr = absl::any_cast<const ReshapeAttributes&>(ctx.op_attr);
     if (attr.new_shape.h != ctx.output_shapes[0][1] ||
         attr.new_shape.w != ctx.output_shapes[0][2] ||
         attr.new_shape.c != ctx.output_shapes[0][3]) {
diff --git a/tensorflow/lite/delegates/gpu/gl/kernels/resize.cc b/tensorflow/lite/delegates/gpu/gl/kernels/resize.cc
index 04485059..f2c96d69 100644
--- a/tensorflow/lite/delegates/gpu/gl/kernels/resize.cc
+++ b/tensorflow/lite/delegates/gpu/gl/kernels/resize.cc
@@ -38,7 +38,7 @@ class Resize : public NodeShader {
  public:
   absl::Status GenerateCode(const GenerationContext& ctx,
                             GeneratedCode* generated_code) const final {
-    const auto& attr = std::any_cast<const Resize2DAttributes&>(ctx.op_attr);
+    const auto& attr = absl::any_cast<const Resize2DAttributes&>(ctx.op_attr);
 
     if (ctx.input_shapes[0][2] > ctx.output_shapes[0][2] ||
         ctx.input_shapes[0][1] > ctx.output_shapes[0][1]) {
diff --git a/tensorflow/lite/delegates/gpu/gl/kernels/slice.cc b/tensorflow/lite/delegates/gpu/gl/kernels/slice.cc
index b0874658..48e98c35 100644
--- a/tensorflow/lite/delegates/gpu/gl/kernels/slice.cc
+++ b/tensorflow/lite/delegates/gpu/gl/kernels/slice.cc
@@ -38,7 +38,7 @@ class Slice : public NodeShader {
  public:
   absl::Status GenerateCode(const GenerationContext& ctx,
                             GeneratedCode* generated_code) const final {
-    const auto& attr = std::any_cast<const SliceAttributes&>(ctx.op_attr);
+    const auto& attr = absl::any_cast<const SliceAttributes&>(ctx.op_attr);
 
     const int4 channels(attr.starts.c, attr.strides.c, attr.ends.c, 0);
     const int4 heights(attr.starts.h, attr.strides.h, attr.ends.h, 0);
diff --git a/tensorflow/lite/delegates/gpu/gl/kernels/softmax.cc b/tensorflow/lite/delegates/gpu/gl/kernels/softmax.cc
index b83dcead..6ad7d607 100644
--- a/tensorflow/lite/delegates/gpu/gl/kernels/softmax.cc
+++ b/tensorflow/lite/delegates/gpu/gl/kernels/softmax.cc
@@ -44,7 +44,7 @@ class Softmax : public NodeShader {
  public:
   absl::Status GenerateCode(const GenerationContext& ctx,
                             GeneratedCode* generated_code) const final {
-    const auto& attr = std::any_cast<const SoftmaxAttributes&>(ctx.op_attr);
+    const auto& attr = absl::any_cast<const SoftmaxAttributes&>(ctx.op_attr);
     if (ctx.input_shapes[0] != ctx.output_shapes[0]) {
       return absl::InvalidArgumentError(
           "Input and output shapes do not match.");
diff --git a/tensorflow/lite/delegates/gpu/gl/kernels/space_to_depth.cc b/tensorflow/lite/delegates/gpu/gl/kernels/space_to_depth.cc
index 60f66d86..f2f0ad85 100644
--- a/tensorflow/lite/delegates/gpu/gl/kernels/space_to_depth.cc
+++ b/tensorflow/lite/delegates/gpu/gl/kernels/space_to_depth.cc
@@ -36,7 +36,7 @@ class SpaceToDepth : public NodeShader {
   absl::Status GenerateCode(const GenerationContext& ctx,
                             GeneratedCode* generated_code) const final {
     const auto& attr =
-        std::any_cast<const SpaceToDepthAttributes&>(ctx.op_attr);
+        absl::any_cast<const SpaceToDepthAttributes&>(ctx.op_attr);
     std::string code = R"(
       for (int i = 0; i < 4; ++i) {
         int dst_c = 4 * gid.z + i;
@@ -70,7 +70,7 @@ class DepthToSpace : public NodeShader {
   absl::Status GenerateCode(const GenerationContext& ctx,
                             GeneratedCode* generated_code) const final {
     const auto& attr =
-        std::any_cast<const SpaceToDepthAttributes&>(ctx.op_attr);
+        absl::any_cast<const SpaceToDepthAttributes&>(ctx.op_attr);
     std::string code = R"(
       for (int i = 0; i < 4; ++i) {
         int dst_c = 4 * gid.z + i;
diff --git a/tensorflow/lite/delegates/gpu/gl/kernels/transpose_conv.cc b/tensorflow/lite/delegates/gpu/gl/kernels/transpose_conv.cc
index 170e5e39..1a8a4126 100644
--- a/tensorflow/lite/delegates/gpu/gl/kernels/transpose_conv.cc
+++ b/tensorflow/lite/delegates/gpu/gl/kernels/transpose_conv.cc
@@ -45,7 +45,7 @@ class ConvolutionTransposedBuffers : public NodeShader {
           "Convolution Transposed does not support more than 1 runtime tensor");
     }
     const auto& attr =
-        std::any_cast<const ConvolutionTransposedAttributes&>(ctx.op_attr);
+        absl::any_cast<const ConvolutionTransposedAttributes&>(ctx.op_attr);
     auto weights = attr.weights.shape;
 
     std::vector<Variable> parameters = {
