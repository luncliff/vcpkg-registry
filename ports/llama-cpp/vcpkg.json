{
  "name": "llama-cpp",
  "version": "4644",
  "description": "LLM inference in C/C++",
  "homepage": "https://github.com/ggerganov/llama.cpp",
  "license": "MIT",
  "dependencies": [
    "curl",
    {
      "name": "vcpkg-cmake",
      "host": true
    },
    {
      "name": "vcpkg-cmake-config",
      "host": true
    }
  ],
  "features": {
    "cuda": {
      "description": "Use CUDA",
      "dependencies": [
        "cudnn"
      ]
    },
    "opencl": {
      "description": "Use OpenCL",
      "dependencies": [
        "opencl"
      ]
    },
    "openmp": {
      "description": "Use OpenMP",
      "dependencies": [
        "openmp"
      ]
    },
    "server": {
      "description": "Build llama server",
      "supports": "windows | linux | osx",
      "dependencies": [
        "curl"
      ]
    },
    "vulkan": {
      "description": "Use Vulkan",
      "dependencies": [
        "vulkan"
      ]
    }
  }
}
