{
  "name": "llama-cpp",
  "version": "4644",
  "description": "LLM inference in C/C++",
  "homepage": "https://github.com/ggerganov/llama.cpp",
  "license": "MIT",
  "dependencies": [
    {
      "name": "curl",
      "default-features": false
    },
    "nlohmann-json",
    {
      "name": "vcpkg-cmake",
      "host": true
    },
    {
      "name": "vcpkg-cmake-config",
      "host": true
    }
  ],
  "default-features": [
    {
      "name": "metal",
      "platform": "osx | ios"
    }
  ],
  "features": {
    "cuda": {
      "description": "Use CUDA",
      "dependencies": [
        "cudnn"
      ]
    },
    "metal": {
      "description": "Use Metal",
      "supports": "osx | ios"
    },
    "opencl": {
      "description": "Use OpenCL",
      "dependencies": [
        "opencl"
      ]
    },
    "openmp": {
      "description": "Use OpenMP",
      "dependencies": [
        "openmp"
      ]
    },
    "server": {
      "description": "Build llama server",
      "supports": "windows | linux | osx",
      "dependencies": [
        {
          "name": "curl",
          "features": [
            "ssl"
          ]
        },
        "openssl"
      ]
    },
    "vulkan": {
      "description": "Use Vulkan",
      "dependencies": [
        {
          "name": "glslang",
          "host": true,
          "features": [
            "tools"
          ]
        },
        "vulkan"
      ]
    }
  }
}
